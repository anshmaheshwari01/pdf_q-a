# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1za0WdOGHQ-QJRavabH2RsC4Emyzl_guU
"""

# app.py
import streamlit as st
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from transformers import pipeline

# Load the PDF once
@st.cache_resource(show_spinner="Loading PDF and building model...")
def initialize_qa():
    loader = PyMuPDFLoader("Nonfiction Reading Test Black Friday.pdf")
    documents = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_documents(documents)

    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = Chroma.from_documents(chunks, embedding=embedding)
    retriever = db.as_retriever()

    qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

    return retriever, qa_pipeline

st.set_page_config(page_title="üìÑ PDF Q/A Chatbot", layout="centered")
st.title("üìò Ask Questions from the Black Friday PDF")

# Load retriever and pipeline
retriever, qa_pipeline = initialize_qa()

# User input
query = st.text_input("‚ùì Ask your question here:")

if st.button("Get Answer") and query:
    with st.spinner("üîç Finding the answer..."):
        docs = retriever.get_relevant_documents(query)
        context = "\n\n".join(doc.page_content for doc in docs[:3])

        if not context.strip():
            st.warning("‚ùå No relevant content found.")
        else:
            result = qa_pipeline({
                "question": query,
                "context": context
            })

            st.success(f"ü§ñ Answer: {result['answer']}")