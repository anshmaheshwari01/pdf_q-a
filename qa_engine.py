# -*- coding: utf-8 -*-
"""backend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PN2SBEXsNNwH5buvazw4AdfAj_VkP1jU
"""

pip install langchain langchain-community langchainhub chromadb pypdf groq pymupdf

pip install --upgrade langchain

!pip install -U langchain-groq



# qa_engine.py
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from transformers import pipeline

class PDF_QA:
    def __init__(self):
        pdf_path = "Nonfiction Reading Test Black Friday.pdf"

        loader = PyMuPDFLoader(pdf_path)
        documents = loader.load()

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(documents)

        embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        db = Chroma.from_documents(chunks, embedding=embedding)
        self.retriever = db.as_retriever()
        self.qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

    def ask(self, question):
        docs = self.retriever.get_relevant_documents(question)
        context = "\n\n".join(doc.page_content for doc in docs[:3])

        if not context.strip():
            return "‚ùå No relevant content found."

        result = self.qa_pipeline({
            "question": question,
            "context": context
        })

        return result["answer"]