# -*- coding: utf-8 -*-
"""backend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PN2SBEXsNNwH5buvazw4AdfAj_VkP1jU
"""

pip install langchain langchain-community langchainhub chromadb pypdf groq pymupdf

pip install --upgrade langchain

!pip install -U langchain-groq



# Install packages if running in Colab (remove if not needed)
# !pip install langchain langchain-community langchainhub chromadb pypdf groq pymupdf
# !pip install --upgrade langchain
# !pip install -U langchain-groq

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from transformers import pipeline

# Load and prepare PDF data
loader = PyMuPDFLoader("Nonfiction Reading Test Black Friday.pdf")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(documents)

embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = Chroma.from_documents(chunks, embedding=embedding)
retriever = db.as_retriever()

# Stronger QA model
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Callable function to be used by Streamlit UI
def answer_question(query: str):
    docs = retriever.get_relevant_documents(query)
    context = "\n\n".join(doc.page_content for doc in docs[:3])

    if not context.strip():
        return "No relevant content found."

    result = qa_pipeline({
        "question": query,
        "context": context
    })

    return result['answer']
